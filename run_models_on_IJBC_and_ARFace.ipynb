{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T12:16:01.581503Z",
     "start_time": "2018-08-06T12:16:01.377552Z"
    }
   },
   "outputs": [],
   "source": [
    "# # see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# %matplotlib inline\n",
    "# import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T12:16:02.981513Z",
     "start_time": "2018-08-06T12:16:01.884463Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as op\n",
    "import argparse\n",
    "from data.data_pipe import get_val_pair\n",
    "from torchvision import transforms as trans\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "from Learner import face_learner\n",
    "from config import get_config\n",
    "from utils import hflip_batch, cosineDim1\n",
    "from data.datasets import IJBCAllCroppedFacesDataset, IJBCVerificationPathDataset, ARVerificationAllPathDataset\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch_original(batch, tta=False):\n",
    "    if tta:\n",
    "        fliped = hflip_batch(batch)\n",
    "        feat_orig = learner.model.get_original_feature(batch.to(conf.device))\n",
    "        feat_flip = learner.model.get_original_feature(fliped.to(conf.device))\n",
    "        feat = (feat_orig + feat_flip) / 2\n",
    "    else:\n",
    "        feat = learner.model.get_original_feature(batch.to(conf.device))\n",
    "    return feat\n",
    "\n",
    "\n",
    "def process_batch_xcos(batch, tta=False):\n",
    "    if tta:\n",
    "        fliped = hflip_batch(batch)\n",
    "        flattened_feature1, feat_map1 = learner.model(batch.to(conf.device))\n",
    "        flattened_feature2, feat_map2 = learner.model(fliped.to(conf.device))\n",
    "        feat_map = (feat_map1 + feat_map2) / 2\n",
    "        flattened_feature = (flattened_feature1 + flattened_feature2) / 2\n",
    "    else:\n",
    "        flattened_feature, feat_map = learner.model(batch.to(conf.device))\n",
    "    return flattened_feature, feat_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Learner and conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fixed_str': 'ir_se50.pth', 'pretrainedMdl': 'ir_se50.pth', 'data_path': PosixPath('data'), 'work_path': PosixPath('work_space'), 'model_path': PosixPath('work_space/models'), 'log_path': PosixPath('work_space/log'), 'save_path': PosixPath('work_space/save'), 'exp_title': 'xCos', 'exp_comment': 'expMS1M', 'input_size': [112, 112], 'embedding_size': 1568, 'use_mobilfacenet': False, 'net_depth': 50, 'drop_ratio': 0.6, 'net_mode': 'ir_se', 'device': device(type='cuda', index=0), 'test_transform': Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
      "), 'data_mode': 'emore', 'vgg_folder': PosixPath('data/faces_vgg_112x112'), 'ms1m_folder': PosixPath('data/faces_ms1m_112x112'), 'emore_folder': PosixPath('data/faces_emore'), 'batch_size': 20, 'USE_SOFTMAX': True, 'SOFTMAX_T': 1, 'facebank_path': PosixPath('data/facebank'), 'threshold': 1.5, 'threshold_xCos': 0.2338, 'face_limit': 10, 'min_face_size': 30}\n",
      "ir_se_50 model generated\n"
     ]
    }
   ],
   "source": [
    "conf = get_config(training=False)\n",
    "conf.batch_size=20 # Why bs_size can only be the number that divide 6000 well?\n",
    "learner = face_learner(conf, inference=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process IJB-C data and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = torch.utils.data.DataLoader(\n",
    "    IJBCAllCroppedFacesDataset('/tmp3/zhe2325138/IJB/IJB-C/'),\n",
    "    batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.load_state(conf, 'ir_se50.pth', model_only=True, from_save_folder=True, strict=False, model_atten=False)\n",
    "learner.model.eval()\n",
    "learner.model.returnGrid = True  # Remember to reset this before return!\n",
    "learner.model_attention.eval()\n",
    "\n",
    "dst_dir = './saved_features/IJB-C/ir_se50_original/'\n",
    "os.makedirs(op.join(dst_dir, 'img'), exist_ok=True)\n",
    "os.makedirs(op.join(dst_dir, 'frames'), exist_ok=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(loader):\n",
    "        tensor = batch['tensor']\n",
    "        src_path = batch['path']\n",
    "        if i % 200 == 0:\n",
    "            print(f'Processing {src_path[0]}')\n",
    "        target_path = op.join(dst_dir, *src_path[0].split('/')[-2:]) + '.npy'\n",
    "        if op.exists(target_path):\n",
    "            if i % 200 == 0:\n",
    "                print(f\"Skipping {target_path} because it exists.\")\n",
    "            continue\n",
    "        feat = process_batch_original(tensor, tta=True)\n",
    "        np.save(target_path, feat.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T08:16:48.699975Z",
     "start_time": "2018-08-05T08:16:48.577620Z"
    }
   },
   "source": [
    "## Our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Runing on model {model_name} --------\n",
      "Processing /tmp3/zhe2325138/IJB/IJB-C/cropped_faces/img/10001_101093.jpg \n",
      "Saving to ./saved_features/IJB-C/2019-09-02-08-21_accuracy:0.9968333333333333_step:436692_CosFace/img/10001_101093.jpg.npz\n",
      "Skipping ./saved_features/IJB-C/2019-09-02-08-21_accuracy:0.9968333333333333_step:436692_CosFace/img/10001_101093.jpg.npz because it exists.\n",
      "Processing /tmp3/zhe2325138/IJB/IJB-C/cropped_faces/img/12624_114173.jpg \n",
      "Saving to ./saved_features/IJB-C/2019-09-02-08-21_accuracy:0.9968333333333333_step:436692_CosFace/img/12624_114173.jpg.npz\n",
      "Processing /tmp3/zhe2325138/IJB/IJB-C/cropped_faces/img/13041_117728.jpg \n",
      "Saving to ./saved_features/IJB-C/2019-09-02-08-21_accuracy:0.9968333333333333_step:436692_CosFace/img/13041_117728.jpg.npz\n",
      "Processing /tmp3/zhe2325138/IJB/IJB-C/cropped_faces/img/13456_121015.jpg \n",
      "Saving to ./saved_features/IJB-C/2019-09-02-08-21_accuracy:0.9968333333333333_step:436692_CosFace/img/13456_121015.jpg.npz\n",
      "Processing /tmp3/zhe2325138/IJB/IJB-C/cropped_faces/img/14096_143826.jpg \n",
      "Saving to ./saved_features/IJB-C/2019-09-02-08-21_accuracy:0.9968333333333333_step:436692_CosFace/img/14096_143826.jpg.npz\n",
      "Processing /tmp3/zhe2325138/IJB/IJB-C/cropped_faces/img/26997_447093.jpg \n",
      "Saving to ./saved_features/IJB-C/2019-09-02-08-21_accuracy:0.9968333333333333_step:436692_CosFace/img/26997_447093.jpg.npz\n",
      "Processing /tmp3/zhe2325138/IJB/IJB-C/cropped_faces/img/27428_452171.jpg \n",
      "Saving to ./saved_features/IJB-C/2019-09-02-08-21_accuracy:0.9968333333333333_step:436692_CosFace/img/27428_452171.jpg.npz\n",
      "Processing /tmp3/zhe2325138/IJB/IJB-C/cropped_faces/img/27852_448043.jpg \n",
      "Saving to ./saved_features/IJB-C/2019-09-02-08-21_accuracy:0.9968333333333333_step:436692_CosFace/img/27852_448043.jpg.npz\n",
      "Processing /tmp3/zhe2325138/IJB/IJB-C/cropped_faces/img/28290_450475.jpg \n",
      "Saving to ./saved_features/IJB-C/2019-09-02-08-21_accuracy:0.9968333333333333_step:436692_CosFace/img/28290_450475.jpg.npz\n",
      "Processing /tmp3/zhe2325138/IJB/IJB-C/cropped_faces/img/28733_447056.jpg \n",
      "Saving to ./saved_features/IJB-C/2019-09-02-08-21_accuracy:0.9968333333333333_step:436692_CosFace/img/28733_447056.jpg.npz\n",
      "Processing /tmp3/zhe2325138/IJB/IJB-C/cropped_faces/img/406_4420.jpg \n",
      "Saving to ./saved_features/IJB-C/2019-09-02-08-21_accuracy:0.9968333333333333_step:436692_CosFace/img/406_4420.jpg.npz\n",
      "Processing /tmp3/zhe2325138/IJB/IJB-C/cropped_faces/img/9911_106608.jpg \n",
      "Saving to ./saved_features/IJB-C/2019-09-02-08-21_accuracy:0.9968333333333333_step:436692_CosFace/img/9911_106608.jpg.npz\n",
      "Processing /tmp3/zhe2325138/IJB/IJB-C/cropped_faces/frames/1079_16518.jpg \n",
      "Saving to ./saved_features/IJB-C/2019-09-02-08-21_accuracy:0.9968333333333333_step:436692_CosFace/frames/1079_16518.jpg.npz\n",
      "Processing /tmp3/zhe2325138/IJB/IJB-C/cropped_faces/frames/12058_430813.jpg \n",
      "Saving to ./saved_features/IJB-C/2019-09-02-08-21_accuracy:0.9968333333333333_step:436692_CosFace/frames/12058_430813.jpg.npz\n",
      "Processing /tmp3/zhe2325138/IJB/IJB-C/cropped_faces/frames/1244_35562.jpg \n",
      "Saving to ./saved_features/IJB-C/2019-09-02-08-21_accuracy:0.9968333333333333_step:436692_CosFace/frames/1244_35562.jpg.npz\n"
     ]
    }
   ],
   "source": [
    "def run_ours_IJBC(model_name):\n",
    "    learner.load_state(\n",
    "        conf, f'{model_name}.pth',\n",
    "        model_only=True, from_save_folder=True, strict=True, model_atten=True)\n",
    "    learner.model.eval()\n",
    "    learner.model.returnGrid = True  # Remember to reset this before return!\n",
    "    learner.model_attention.eval()\n",
    "\n",
    "    dst_dir = f'./saved_features/IJB-C/{model_name}/'\n",
    "    os.makedirs(op.join(dst_dir, 'img'), exist_ok=True)\n",
    "    os.makedirs(op.join(dst_dir, 'frames'), exist_ok=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(loader):\n",
    "            tensor = batch['tensor']\n",
    "            src_path = batch['path']\n",
    "            target_path = op.join(dst_dir, *src_path[0].split('/')[-2:]) + '.npz'\n",
    "            if i % 2000 == 0:\n",
    "                print(f'Processing {src_path[0]} \\nSaving to {target_path}')\n",
    "            if op.exists(target_path):\n",
    "                if i % 2000 == 0:\n",
    "                    print(f\"Skipping {target_path} because it exists.\")\n",
    "                continue\n",
    "            flattened_feature, feat_map = process_batch_xcos(tensor, tta=True)\n",
    "            np.savez(target_path, flattened_feature=flattened_feature.cpu().numpy(),\n",
    "                     feat_map=feat_map.cpu().numpy())\n",
    "\n",
    "\n",
    "model_names = [\n",
    "#     '2019-09-01-15-30_accuracy:0.9946666666666667_step:218346_CosFace',\n",
    "    '2019-09-02-08-21_accuracy:0.9968333333333333_step:436692_CosFace',\n",
    "    '2019-08-25-14-35_accuracy:0.9931666666666666_step:218349_None',\n",
    "    '2019-08-30-07-36_accuracy:0.9953333333333333_step:655047_None'\n",
    "]\n",
    "for model_name in model_names:\n",
    "    print('-------- Runing on model {model_name} --------')\n",
    "    run_ours_IJBC(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process ARFace data and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = torch.utils.data.DataLoader(ARVerificationAllPathDataset(), batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.load_state(conf, 'ir_se50.pth', model_only=True, from_save_folder=True, strict=False, model_atten=False)\n",
    "learner.model.eval()\n",
    "learner.model.returnGrid = True  # Remember to reset this before return!\n",
    "learner.model_attention.eval()\n",
    "\n",
    "dst_dir = './saved_features/ARFace/ir_se50_original/'\n",
    "os.makedirs(dst_dir, exist_ok=True)\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(loader):\n",
    "        tensor = batch['image_tensor']\n",
    "        fname = batch['fname'][0]\n",
    "        target_path = op.join(dst_dir, fname) + '.npy'\n",
    "        if i % 200 == 0:\n",
    "            print(f'Processing {fname}, saving to {target_path}')\n",
    "        if op.exists(target_path):\n",
    "            if i % 200 == 0:\n",
    "                print(f\"Skipping {target_path} because it exists.\")\n",
    "            continue\n",
    "        feat = process_batch_original(tensor, tta=True)\n",
    "        np.save(target_path, feat.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_our_model(name):\n",
    "    learner.load_state(conf, f'{name}.pth', model_only=True, from_save_folder=True, strict=False, model_atten=False)\n",
    "    learner.model.eval()\n",
    "    learner.model.returnGrid = True  # Remember to reset this before return!\n",
    "    learner.model_attention.eval()\n",
    "\n",
    "    dst_dir = f'./saved_features/ARFace/{name}/'\n",
    "    os.makedirs(dst_dir, exist_ok=True)\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(loader):\n",
    "            tensor = batch['image_tensor']\n",
    "            fname = batch['fname'][0]\n",
    "            target_path = op.join(dst_dir, fname) + '.npz'\n",
    "            if i % 200 == 0:\n",
    "                print(f'Processing {fname}, saving to {target_path}')\n",
    "            if op.exists(target_path):\n",
    "                if i % 200 == 0:\n",
    "                    print(f\"Skipping {target_path} because it exists.\")\n",
    "                continue\n",
    "            flattened_feature, feat_map = process_batch_xcos(tensor, tta=True)\n",
    "            np.savez(target_path, flattened_feature=flattened_feature.cpu().numpy(),\n",
    "                     feat_map=feat_map.cpu().numpy())\n",
    "            \n",
    "names = [\n",
    "    '2019-09-02-08-21_accuracy:0.9968333333333333_step:436692_CosFace',\n",
    "    '2019-08-25-14-35_accuracy:0.9931666666666666_step:218349_None',\n",
    "    '2019-08-30-07-36_accuracy:0.9953333333333333_step:655047_None',\n",
    "#     '2019-09-01-15-30_accuracy:0.9946666666666667_step:218346_CosFace',\n",
    "]\n",
    "for name in names:\n",
    "    print(f'---------------------- Testing model {name} ----------------------')\n",
    "    compute_our_model(name)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "xCos",
   "language": "python",
   "name": "xcos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
